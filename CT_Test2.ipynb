{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "jXlAy_vmgqdp"
      ],
      "gpuType": "T4",
      "mount_file_id": "1RQgOIbpAxv2w9CG1NJAlyKQ7CJV9I6BF",
      "authorship_tag": "ABX9TyMBqipGuKpkTST/Tyl5CRh4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Astro2350/CT-Train/blob/main/CT_Test2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preprocess"
      ],
      "metadata": {
        "id": "jXlAy_vmgqdp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import os\n",
        "import pickle\n",
        "\n",
        "# NLTK setup for stopwords and lemmatization\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "# Function to preprocess text (remove stopwords, lemmatize, clean text)\n",
        "def preprocess_text(text):\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    text = text.lower()  # Lowercase\n",
        "    text = re.sub(r'[^a-zA-Z\\s]', '', text)  # Remove non-alphabetical characters\n",
        "    text = ' '.join([lemmatizer.lemmatize(word) for word in text.split() if word not in stop_words])\n",
        "    return text\n",
        "\n",
        "# Load raw datasets (they should be in /content/)\n",
        "categories_df = pd.read_csv('list_of_categories.csv')\n",
        "train_df = pd.read_csv('TRAIN.csv')\n",
        "\n",
        "print(\"Preprocessing data...\")\n",
        "\n",
        "# Clean column names\n",
        "train_df.columns = train_df.columns.str.strip().str.lower().str.replace(' ', '_')\n",
        "categories_df.columns = categories_df.columns.str.strip().str.lower().str.replace(' ', '_')\n",
        "\n",
        "# Merge datasets to include category information\n",
        "merged_df = train_df.merge(categories_df, left_on='primary_category_id', right_on='id', how='left')\n",
        "\n",
        "# Feature engineering: create combined_text\n",
        "merged_df['name'] = merged_df['name'].fillna('')\n",
        "merged_df['gl_description'] = merged_df['gl_description'].fillna('')\n",
        "merged_df['memo'] = merged_df['memo'].fillna('')\n",
        "merged_df['combined_text'] = merged_df['name'] + ' ' + merged_df['gl_description'] + ' ' + merged_df['memo']\n",
        "\n",
        "# Preprocess text in combined_text column\n",
        "merged_df['combined_text'] = merged_df['combined_text'].apply(preprocess_text)\n",
        "\n",
        "# Encode target variable using LabelEncoder (temporary encoding)\n",
        "label_encoder = LabelEncoder()\n",
        "merged_df['matched_category_id_encoded'] = label_encoder.fit_transform(merged_df['matched_category_id'])\n",
        "print(f\"Number of target classes before filtering: {len(label_encoder.classes_)}\")\n",
        "\n",
        "# Handle categorical features\n",
        "merged_df['hospital_system_id'] = merged_df['hospital_system_id'].astype(str)\n",
        "merged_df['hospital_system_id_encoded'] = LabelEncoder().fit_transform(merged_df['hospital_system_id'])\n",
        "\n",
        "merged_df['department_name'] = merged_df['department_name'].fillna('Unknown')\n",
        "merged_df['department_name_encoded'] = LabelEncoder().fit_transform(merged_df['department_name'])\n",
        "\n",
        "# Handle category hierarchy features (for 6 levels)\n",
        "for i in range(6):\n",
        "    col_name = f'category{i}'\n",
        "    merged_df[col_name] = merged_df[col_name].fillna('Unknown')\n",
        "    merged_df[f'{col_name}_encoded'] = LabelEncoder().fit_transform(merged_df[col_name])\n",
        "\n",
        "# Remove classes with only one sample\n",
        "class_counts = merged_df['matched_category_id_encoded'].value_counts()\n",
        "rare_classes = class_counts[class_counts == 1].index\n",
        "filtered_df = merged_df[~merged_df['matched_category_id_encoded'].isin(rare_classes)].copy()  # Use .copy() to avoid warning\n",
        "\n",
        "# Re-fit the label encoder on the filtered data so that labels become contiguous\n",
        "label_encoder = LabelEncoder()\n",
        "filtered_df['matched_category_id_encoded'] = label_encoder.fit_transform(filtered_df['matched_category_id'])\n",
        "num_classes = filtered_df['matched_category_id_encoded'].nunique()\n",
        "print(f\"Number of classes after filtering: {num_classes}\")\n",
        "\n",
        "# Save the processed data\n",
        "filtered_df.to_csv('/content/processed_train_data.csv', index=False)\n",
        "\n",
        "# Save the re-fitted label encoder for later use in training\n",
        "with open('/content/label_encoder.pkl', 'wb') as f:\n",
        "    pickle.dump(label_encoder, f)\n",
        "\n",
        "print(\"Data preprocessing complete and saved to '/content/processed_train_data.csv'!\")"
      ],
      "metadata": {
        "id": "8Pt3dDlYgpUB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training"
      ],
      "metadata": {
        "id": "2Jg-cwYZgvBJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import DistilBertTokenizer, DistilBertModel\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torch.optim.lr_scheduler import StepLR\n",
        "from torch.cuda.amp import autocast, GradScaler\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "import pickle\n",
        "from tqdm.notebook import tqdm\n",
        "import os\n",
        "import csv\n",
        "import json\n",
        "\n",
        "# Load processed data and label encoder\n",
        "train_df = pd.read_csv('/content/processed_train_data.csv')\n",
        "with open('/content/label_encoder.pkl', 'rb') as f:\n",
        "    label_encoder = pickle.load(f)\n",
        "\n",
        "# Determine number of classes from processed data\n",
        "num_classes = train_df['matched_category_id_encoded'].nunique()\n",
        "print(f\"Number of classes from processed data: {num_classes}\")\n",
        "\n",
        "# Load pre-trained model and tokenizer\n",
        "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
        "bert_model = DistilBertModel.from_pretrained('distilbert-base-uncased')\n",
        "\n",
        "# Custom dataset class\n",
        "class TransactionDataset(Dataset):\n",
        "    def __init__(self, dataframe, tokenizer, max_length=128):\n",
        "        self.data = dataframe\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        row = self.data.iloc[index]\n",
        "        text = row['combined_text']\n",
        "        encoding = self.tokenizer(\n",
        "            text,\n",
        "            max_length=self.max_length,\n",
        "            padding='max_length',\n",
        "            truncation=True,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "        input_ids = encoding['input_ids'].squeeze()\n",
        "        attention_mask = encoding['attention_mask'].squeeze()\n",
        "\n",
        "        # Combine categorical features: hospital_system_id_encoded, primary_category_id, department_name_encoded\n",
        "        cat_features = torch.tensor([row['hospital_system_id_encoded'], row['primary_category_id'], row['department_name_encoded']], dtype=torch.long)\n",
        "\n",
        "        # Hierarchical features (6 columns)\n",
        "        hier_features = torch.tensor([row['category0_encoded'], row['category1_encoded'], row['category2_encoded'],\n",
        "                                      row['category3_encoded'], row['category4_encoded'], row['category5_encoded']], dtype=torch.long)\n",
        "\n",
        "        # Amount feature\n",
        "        amount = torch.tensor([row['amount']], dtype=torch.float)\n",
        "\n",
        "        # Target label\n",
        "        target = torch.tensor(row['matched_category_id_encoded'], dtype=torch.long)\n",
        "\n",
        "        return {\n",
        "            'input_ids': input_ids,\n",
        "            'attention_mask': attention_mask,\n",
        "            'cat_features': cat_features,\n",
        "            'hier_features': hier_features,\n",
        "            'amount': amount,\n",
        "            'target': target\n",
        "        }\n",
        "\n",
        "# Split the processed data into training and validation sets\n",
        "train_data, val_data = train_test_split(train_df, test_size=0.2, random_state=42, stratify=train_df['matched_category_id_encoded'])\n",
        "\n",
        "# Create DataLoaders\n",
        "batch_size = 16\n",
        "train_dataset = TransactionDataset(train_data, tokenizer)\n",
        "val_dataset = TransactionDataset(val_data, tokenizer)\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
        "\n",
        "# Define the model\n",
        "class HierarchicalCategoryModel(nn.Module):\n",
        "    def __init__(self, bert_model, num_cat_features=3, num_hier_features=6, num_classes=357):\n",
        "        super(HierarchicalCategoryModel, self).__init__()\n",
        "        self.bert = bert_model\n",
        "        self.bert_dropout = nn.Dropout(0.1)\n",
        "        self.bert_dim = 768  # DistilBERT hidden size\n",
        "\n",
        "        # Embeddings for categorical features\n",
        "        self.cat_embeddings = nn.ModuleList([nn.Embedding(10000, 32) for _ in range(num_cat_features)])\n",
        "        self.hier_embeddings = nn.ModuleList([nn.Embedding(10000, 32) for _ in range(num_hier_features)])\n",
        "\n",
        "        # Calculate total embedding dimensions\n",
        "        self.cat_emb_dim = 32 * num_cat_features\n",
        "        self.hier_emb_dim = 32 * num_hier_features\n",
        "\n",
        "        # Total input dimension: BERT output + categorical embeddings + hierarchical embeddings + amount\n",
        "        self.total_input_dim = self.bert_dim + self.cat_emb_dim + self.hier_emb_dim + 1\n",
        "\n",
        "        # Fully connected layers\n",
        "        self.fc1 = nn.Linear(self.total_input_dim, 512)\n",
        "        self.fc2 = nn.Linear(512, 256)\n",
        "        self.fc3 = nn.Linear(256, num_classes)  # Set to match number of classes\n",
        "\n",
        "        self.dropout = nn.Dropout(0.2)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.batch_norm1 = nn.BatchNorm1d(512)\n",
        "        self.batch_norm2 = nn.BatchNorm1d(256)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask, cat_features, hier_features, amount):\n",
        "        # Process BERT output\n",
        "        bert_output = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        bert_cls = bert_output.last_hidden_state[:, 0, :]\n",
        "        bert_cls = self.bert_dropout(bert_cls)\n",
        "\n",
        "        # Process categorical features\n",
        "        cat_embeddings = [emb(cat_features[:, i]) for i, emb in enumerate(self.cat_embeddings)]\n",
        "        cat_embeddings = torch.cat(cat_embeddings, dim=1)\n",
        "\n",
        "        # Process hierarchical features\n",
        "        hier_embeddings = [emb(hier_features[:, i]) for i, emb in enumerate(self.hier_embeddings)]\n",
        "        hier_embeddings = torch.cat(hier_embeddings, dim=1)\n",
        "\n",
        "        # Concatenate all features\n",
        "        combined = torch.cat([bert_cls, cat_embeddings, hier_embeddings, amount], dim=1)\n",
        "\n",
        "        # Fully connected layers with batch normalization and dropout\n",
        "        x = self.fc1(combined)\n",
        "        x = self.batch_norm1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.dropout(x)\n",
        "\n",
        "        x = self.fc2(x)\n",
        "        x = self.batch_norm2(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.dropout(x)\n",
        "\n",
        "        logits = self.fc3(x)\n",
        "        return logits\n",
        "\n",
        "# Initialize device (CPU or CUDA)\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Use the number of classes from the processed data\n",
        "print(f\"Setting model num_classes to: {num_classes}\")\n",
        "model = HierarchicalCategoryModel(bert_model, num_classes=num_classes).to(device)\n",
        "\n",
        "# Set up optimizer\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5, weight_decay=0.01)\n",
        "\n",
        "# Compute class weights based on the processed training data (make sure to use the training portion)\n",
        "class_weights = compute_class_weight('balanced',\n",
        "                                     classes=np.unique(train_df['matched_category_id_encoded']),\n",
        "                                     y=train_df['matched_category_id_encoded'])\n",
        "class_weights = torch.tensor(class_weights, dtype=torch.float).to(device)\n",
        "criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
        "\n",
        "# Learning rate scheduler\n",
        "scheduler = StepLR(optimizer, step_size=5, gamma=0.1)\n",
        "\n",
        "# Mixed Precision Training setup - if CUDA is not available, this will simply be ignored\n",
        "scaler = GradScaler()\n",
        "\n",
        "# Training function\n",
        "def train_epoch(model, dataloader, optimizer, criterion, device, scaler):\n",
        "    model.train()\n",
        "    epoch_loss = 0\n",
        "    correct_predictions = 0\n",
        "    total_predictions = 0\n",
        "\n",
        "    progress_bar = tqdm(dataloader, desc=\"Training\")\n",
        "\n",
        "    for batch in progress_bar:\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "        cat_features = batch['cat_features'].to(device)\n",
        "        hier_features = batch['hier_features'].to(device)\n",
        "        amount = batch['amount'].to(device)\n",
        "        targets = batch['target'].to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        if device.type == 'cuda':\n",
        "            with autocast(device_type=device.type):\n",
        "                outputs = model(\n",
        "                    input_ids=input_ids,\n",
        "                    attention_mask=attention_mask,\n",
        "                    cat_features=cat_features,\n",
        "                    hier_features=hier_features,\n",
        "                    amount=amount\n",
        "                )\n",
        "                loss = criterion(outputs, targets)\n",
        "            scaler.scale(loss).backward()\n",
        "            scaler.step(optimizer)\n",
        "            scaler.update()\n",
        "        else:\n",
        "            outputs = model(\n",
        "                input_ids=input_ids,\n",
        "                attention_mask=attention_mask,\n",
        "                cat_features=cat_features,\n",
        "                hier_features=hier_features,\n",
        "                amount=amount\n",
        "            )\n",
        "            loss = criterion(outputs, targets)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        _, preds = torch.max(outputs, dim=1)\n",
        "        correct_predictions += torch.sum(preds == targets)\n",
        "        total_predictions += targets.shape[0]\n",
        "        epoch_loss += loss.item()\n",
        "        progress_bar.set_postfix({\"loss\": loss.item(), \"accuracy\": (correct_predictions.float() / total_predictions).item()})\n",
        "\n",
        "    return epoch_loss / len(dataloader), correct_predictions.float() / total_predictions\n",
        "\n",
        "# Evaluation function\n",
        "def evaluate(model, dataloader, criterion, device):\n",
        "    model.eval()\n",
        "    epoch_loss = 0\n",
        "    correct_predictions = 0\n",
        "    total_predictions = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in tqdm(dataloader, desc=\"Evaluation\"):\n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "            attention_mask = batch['attention_mask'].to(device)\n",
        "            cat_features = batch['cat_features'].to(device)\n",
        "            hier_features = batch['hier_features'].to(device)\n",
        "            amount = batch['amount'].to(device)\n",
        "            targets = batch['target'].to(device)\n",
        "\n",
        "            outputs = model(\n",
        "                input_ids=input_ids,\n",
        "                attention_mask=attention_mask,\n",
        "                cat_features=cat_features,\n",
        "                hier_features=hier_features,\n",
        "                amount=amount\n",
        "            )\n",
        "            loss = criterion(outputs, targets)\n",
        "            _, preds = torch.max(outputs, dim=1)\n",
        "            correct_predictions += torch.sum(preds == targets)\n",
        "            total_predictions += targets.shape[0]\n",
        "            epoch_loss += loss.item()\n",
        "\n",
        "    return epoch_loss / len(dataloader), correct_predictions.float() / total_predictions\n",
        "\n",
        "# Training loop with early stopping and checkpoint saving\n",
        "num_epochs = 3\n",
        "best_accuracy = 0\n",
        "patience = 3\n",
        "epochs_no_improve = 0\n",
        "\n",
        "checkpoint_dir = \"/content/drive/MyDrive/checkpoints\"\n",
        "os.makedirs(checkpoint_dir, exist_ok=True)\n",
        "log_filepath = \"/content/drive/MyDrive/training_logs.csv\"\n",
        "\n",
        "with open(log_filepath, mode='w', newline='') as file:\n",
        "    writer = csv.writer(file)\n",
        "    writer.writerow(['Epoch', 'Train Loss', 'Train Accuracy', 'Val Loss', 'Val Accuracy'])\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    print(f\"Epoch {epoch + 1}/{num_epochs}\")\n",
        "\n",
        "    train_loss, train_acc = train_epoch(model, train_loader, optimizer, criterion, device, scaler)\n",
        "    print(f\"Train Loss: {train_loss:.4f}, Train Accuracy: {train_acc:.4f}\")\n",
        "\n",
        "    val_loss, val_acc = evaluate(model, val_loader, criterion, device)\n",
        "    print(f\"Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_acc:.4f}\")\n",
        "\n",
        "    if val_acc > best_accuracy:\n",
        "        best_accuracy = val_acc\n",
        "        epochs_no_improve = 0\n",
        "        checkpoint_path = f\"{checkpoint_dir}/best_model.pth\"\n",
        "        # Save checkpoint\n",
        "        def save_checkpoint(model, optimizer, epoch, loss, accuracy, filepath):\n",
        "            checkpoint = {\n",
        "                'epoch': epoch,\n",
        "                'model_state_dict': model.state_dict(),\n",
        "                'optimizer_state_dict': optimizer.state_dict(),\n",
        "                'loss': loss,\n",
        "                'accuracy': accuracy\n",
        "            }\n",
        "            torch.save(checkpoint, filepath)\n",
        "            print(f\"Checkpoint saved to {filepath}\")\n",
        "        save_checkpoint(model, optimizer, epoch + 1, val_loss, val_acc, checkpoint_path)\n",
        "    else:\n",
        "        epochs_no_improve += 1\n",
        "        if epochs_no_improve >= patience:\n",
        "            print(f\"Early stopping triggered after {epoch + 1} epochs\")\n",
        "            break\n",
        "\n",
        "    with open(log_filepath, mode='a', newline='') as file:\n",
        "        writer = csv.writer(file)\n",
        "        writer.writerow([epoch + 1, train_loss, train_acc.item(), val_loss, val_acc.item()])\n",
        "\n",
        "    scheduler.step()\n",
        "\n",
        "    print()\n",
        "\n",
        "print(f\"Best validation accuracy: {best_accuracy:.4f}\")\n",
        "\n",
        "best_model_path = f\"{checkpoint_dir}/best_model.pth\"\n",
        "if os.path.exists(best_model_path):\n",
        "    checkpoint = torch.load(best_model_path, map_location=device)\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "    print(f\"Loaded best model from epoch {checkpoint['epoch']} with accuracy {checkpoint['accuracy']:.4f}\")\n",
        "else:\n",
        "    print(\"No saved model found. Using the last trained model.\")\n",
        "\n",
        "category_mapping = {i: label for i, label in enumerate(label_encoder.classes_)}\n",
        "with open(f\"{checkpoint_dir}/category_mapping.json\", 'w') as f:\n",
        "    json.dump(category_mapping, f)\n",
        "print(f\"Category mapping saved to {checkpoint_dir}/category_mapping.json\")\n",
        "\n",
        "print(\"Training complete!\")"
      ],
      "metadata": {
        "id": "UmkgF6vXfjJm"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}